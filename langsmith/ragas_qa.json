[
    {
        "question": "What is the purpose of the RAGAS framework?",
        "answer": "The RAGAS framework aims to provide a suite of metrics for automated evaluation of Retrieval Augmented Generation (RAG) systems without relying on ground truth human annotations."
    },
    {
        "question": "What are the limitations of using LLMs as knowledge bases?",
        "answer": "LLMs have limitations as knowledge bases, including their inability to answer questions about events after they were trained and their struggle to memorize long-tail knowledge."
    },
    {
        "question": "How do RAG systems address the limitations of LLMs?",
        "answer": "RAG systems retrieve relevant passages from a corpus and provide them to the LLM, enabling the LLM to answer questions without having to memorize all knowledge."
    },
    {
        "question": "Why is automated evaluation of RAG systems important?",
        "answer": "Automated evaluation is important for RAG systems because there are many dimensions to consider, including retrieval, LLM exploitation, and generation quality. Faster evaluation cycles are crucial given the fast adoption of LLMs."
    },
    {
        "question": "What are the three quality aspects focused on in the RAGAS framework?",
        "answer": "The RAGAS framework focuses on Faithfulness, Answer Relevance, and Context Relevance as key quality aspects of RAG systems."
    },
    {
        "question": "What is Faithfulness?",
        "answer": "Faithfulness measures whether the claims in the answer can be inferred from the retrieved context, aiming to avoid hallucinations and ensure the answer is grounded in the context."
    },
    {
        "question": "What is Answer Relevance?",
        "answer": "Answer Relevance measures whether the generated answer directly addresses the given question and penalizes cases with incomplete or redundant information."
    },
    {
        "question": "What is Context Relevance?",
        "answer": "Context Relevance measures how focused and relevant the retrieved context is, aiming to penalize the inclusion of irrelevant information."
    },
    {
        "question": "How is Faithfulness estimated?",
        "answer": "Faithfulness is estimated by extracting statements from the answer, determining if each statement can be inferred from the context, and computing a faithfulness score based on the proportion of supported statements."
    },
    {
        "question": "How is Answer Relevance estimated?",
        "answer": "Answer Relevance is estimated by generating potential questions based on the answer and calculating the similarity of these questions to the original question. The answer relevance score is based on the average similarity."
    },
    {
        "question": "How is Context Relevance estimated?",
        "answer": "Context Relevance is estimated by extracting sentences from the context that are crucial for answering the question and computing the proportion of extracted sentences to total sentences as the context relevance score."
    },
    {
        "question": "What is the WikiEval dataset used for?",
        "answer": "The WikiEval dataset is used to evaluate the proposed metrics in the RAGAS framework by comparing them against human judgments on question-context-answer triples."
    },
    {
        "question": "How were the questions and answers in WikiEval obtained?",
        "answer": "The questions in WikiEval were suggested by ChatGPT based on the introductory sections of 50 recent Wikipedia pages. The answers were generated by ChatGPT using the corresponding context."
    },
    {
        "question": "How were the quality judgments in WikiEval annotated?",
        "answer": "Quality judgments in WikiEval were annotated by two annotators on the three aspects of Faithfulness, Answer Relevance, and Context Relevance. Disagreements were resolved through discussion."
    }
]
